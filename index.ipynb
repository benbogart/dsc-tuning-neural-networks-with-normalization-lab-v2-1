{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Normalization - Lab \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab you'll build a neural network to perform a regression task.\n",
    "\n",
    "It is worth noting that getting regression to work with neural networks can be comparatively difficult because the output is unbounded ($\\hat y$ can technically range from $-\\infty$ to $+\\infty$), and the models are especially prone to exploding gradients. This issue makes a regression exercise the perfect learning case for tinkering with normalization and optimization strategies to ensure proper convergence!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Fit a neural network to normalized data \n",
    "- Implement and observe the impact of various initialization techniques \n",
    "- Implement and observe the impact of various optimization techniques \n",
    "\n",
    "## Load the data \n",
    "\n",
    "First, run the following cell to import all the neccessary libraries and classes you will need in this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries and classes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll be working with the housing prices data you saw in an earlier section. However, we did a lot of preprocessing for you so you can focus on normalizing numeric features and building neural network models! The following preprocessing steps were taken (all the code can be found in the `data_preprocessing.ipynb` notebook in this repository): \n",
    "\n",
    "- The data was split into the training, validate, and test sets \n",
    "- All the missing values in numeric columns were replaced by the median of those columns \n",
    "- All the missing values in catetgorical columns were replaced with the word 'missing' \n",
    "- All the categorical columns were one-hot encoded \n",
    "\n",
    "Run the following cells to import the train, validate, and test sets:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all numeric features\n",
    "X_train_numeric = pd.read_csv('data/X_train_numeric.csv')\n",
    "X_val_numeric = pd.read_csv('data/X_val_numeric.csv')\n",
    "X_test_numeric = pd.read_csv('data/X_test_numeric.csv')\n",
    "\n",
    "# Load all categorical features\n",
    "X_train_cat = pd.read_csv('data/X_train_cat.csv')\n",
    "X_val_cat = pd.read_csv('data/X_val_cat.csv')\n",
    "X_test_cat = pd.read_csv('data/X_test_cat.csv')\n",
    "\n",
    "# Load all targets\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "y_val = pd.read_csv('data/y_val.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "X_train = pd.concat([X_train_numeric, X_train_cat], axis=1)\n",
    "X_val = pd.concat([X_val_numeric, X_val_cat], axis=1)\n",
    "X_test = pd.concat([X_test_numeric, X_test_cat], axis=1)\n",
    "\n",
    "# Number of features\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, preview the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>21453.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>938.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>12420.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9742.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5389.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>11003.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>765.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 296 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0        80.0         69.0  21453.0          6.0          5.0     1969.0   \n",
       "1        60.0         79.0  12420.0          7.0          5.0     2001.0   \n",
       "2        20.0         75.0   9742.0          8.0          5.0     2002.0   \n",
       "3       120.0         39.0   5389.0          8.0          5.0     1995.0   \n",
       "4        60.0         85.0  11003.0         10.0          5.0     2008.0   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_ConLw  \\\n",
       "0        1969.0         0.0       938.0         0.0  ...             0.0   \n",
       "1        2001.0         0.0       666.0         0.0  ...             0.0   \n",
       "2        2002.0       281.0         0.0         0.0  ...             0.0   \n",
       "3        1996.0         0.0      1180.0         0.0  ...             0.0   \n",
       "4        2008.0       160.0       765.0         0.0  ...             0.0   \n",
       "\n",
       "   SaleType_New  SaleType_Oth  SaleType_WD  SaleCondition_Abnorml  \\\n",
       "0           0.0           0.0          1.0                    0.0   \n",
       "1           0.0           0.0          1.0                    0.0   \n",
       "2           0.0           0.0          1.0                    0.0   \n",
       "3           0.0           0.0          1.0                    0.0   \n",
       "4           0.0           0.0          1.0                    0.0   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                    0.0                   0.0                   0.0   \n",
       "1                    0.0                   0.0                   0.0   \n",
       "2                    0.0                   0.0                   0.0   \n",
       "3                    0.0                   0.0                   0.0   \n",
       "4                    0.0                   0.0                   0.0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  \n",
       "0                   1.0                    0.0  \n",
       "1                   1.0                    0.0  \n",
       "2                   1.0                    0.0  \n",
       "3                   1.0                    0.0  \n",
       "4                   1.0                    0.0  \n",
       "\n",
       "[5 rows x 296 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline Model\n",
    "\n",
    "Building a naive baseline model to compare performance against is a helpful reference point. From there, you can then observe the impact of various tunning procedures which will iteratively improve your model. So, let's do just that! \n",
    "\n",
    "In the cell below: \n",
    "\n",
    "- Add an input layer with `n_features` units \n",
    "- Add two hidden layers, one with 100 and the other with 50 units (make sure you use the `'relu'` activation function) \n",
    "- Add an output layer with 1 unit and `'linear'` activation \n",
    "- Compile and fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 63/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 101/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 102/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 103/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 104/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 105/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 106/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 107/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 108/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 109/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 110/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 111/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 112/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 113/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 114/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 115/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 116/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 117/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 118/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 119/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 120/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 121/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 122/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 123/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 124/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 125/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 126/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 127/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 128/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 129/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 130/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 131/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 132/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 133/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 134/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 135/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 136/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 137/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 138/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 139/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 140/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 141/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 142/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 143/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 144/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 145/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 146/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 147/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 148/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 149/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n",
      "Epoch 150/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: nan - mse: nan - val_loss: nan - val_mse: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f27b9e029d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "baseline_model = Sequential()\n",
    "\n",
    "# Hidden layer with 100 units\n",
    "baseline_model.add(layers.Dense(100, activation='relu', input_shape=(296,)))\n",
    "\n",
    "# Hidden layer with 50 units\n",
    "baseline_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "baseline_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "baseline_model.compile(optimizer='SGD', \n",
    "                       loss='mse', \n",
    "                       metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "baseline_model.fit(X_train, \n",
    "                   y_train, \n",
    "                   batch_size=32, \n",
    "                   epochs=150, \n",
    "                   validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**Notice this extremely problematic behavior: all the values for training and validation loss are \"nan\". This indicates that the algorithm did not converge. The first solution to this is to normalize the input. From there, if convergence is not achieved, normalizing the output may also be required.**_ \n",
    "\n",
    "## Normalize the Input Data \n",
    "\n",
    "It's now time to normalize the input data. In the cell below: \n",
    "\n",
    "- Assign the column names of all numeric columns to `numeric_columns` \n",
    "- Instantiate a `StandardScaler` \n",
    "- Fit and transform `X_train_numeric`. Make sure you convert the result into a DataFrame (use `numeric_columns` as the column names) \n",
    "- Transform validate and test sets (`X_val_numeric` and `X_test_numeric`), and convert these results into DataFrames as well \n",
    "- Use the provided to combine the scaled numerical and categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric column names\n",
    "numeric_columns = X_test_numeric.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "ss_X = StandardScaler()\n",
    "\n",
    "# Fit and transform train data\n",
    "X_train_scaled = pd.DataFrame(ss_X.fit_transform(X_train_numeric),\n",
    "                              columns=numeric_columns)\n",
    "\n",
    "# Transform validate and test data\n",
    "X_val_scaled = pd.DataFrame(ss_X.transform(X_val_numeric),\n",
    "                              columns=numeric_columns)\n",
    "X_test_scaled = pd.DataFrame(ss_X.transform(X_test_numeric),\n",
    "                              columns=numeric_columns)\n",
    "\n",
    "# Combine the scaled numerical features and categorical features\n",
    "X_train = pd.concat([X_train_scaled, X_train_cat], axis=1)\n",
    "X_val = pd.concat([X_val_scaled, X_val_cat], axis=1)\n",
    "X_test = pd.concat([X_test_scaled, X_test_cat], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following cell to compile a neural network model (with the same architecture as before): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1051, 296)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with all normalized inputs\n",
    "np.random.seed(123)\n",
    "normalized_input_model = Sequential()\n",
    "normalized_input_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "normalized_input_model.add(layers.Dense(50, activation='relu'))\n",
    "normalized_input_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "normalized_input_model.compile(optimizer='SGD', \n",
    "                               loss='mse', \n",
    "                               metrics=['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below: \n",
    "- Train the `normalized_input_model` on normalized input (`X_train`) and output (`y_train`) \n",
    "- Set a batch size of 32 and train for 150 epochs \n",
    "- Specify the `validation_data` argument as `(X_val, y_val)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 6179433984.0000 - mse: 6179433984.0000 - val_loss: 5272705024.0000 - val_mse: 5272705024.0000\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179456000.0000 - mse: 6179456000.0000 - val_loss: 5267808256.0000 - val_mse: 5267808256.0000\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178725376.0000 - mse: 6178725376.0000 - val_loss: 5267374080.0000 - val_mse: 5267374080.0000\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179058688.0000 - mse: 6179058688.0000 - val_loss: 5267227136.0000 - val_mse: 5267227136.0000\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178145792.0000 - mse: 6178145792.0000 - val_loss: 5261344768.0000 - val_mse: 5261344768.0000\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179017728.0000 - mse: 6179017728.0000 - val_loss: 5268096000.0000 - val_mse: 5268096000.0000\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179791872.0000 - mse: 6179791872.0000 - val_loss: 5267285504.0000 - val_mse: 5267285504.0000\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177320448.0000 - mse: 6177320448.0000 - val_loss: 5265112576.0000 - val_mse: 5265112576.0000\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180123136.0000 - mse: 6180123136.0000 - val_loss: 5271466496.0000 - val_mse: 5271466496.0000\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179564544.0000 - mse: 6179564544.0000 - val_loss: 5268787200.0000 - val_mse: 5268787200.0000\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178057216.0000 - mse: 6178057216.0000 - val_loss: 5265399808.0000 - val_mse: 5265399808.0000\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178652672.0000 - mse: 6178652672.0000 - val_loss: 5263417344.0000 - val_mse: 5263417344.0000\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178372096.0000 - mse: 6178372096.0000 - val_loss: 5263246848.0000 - val_mse: 5263246848.0000\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178264576.0000 - mse: 6178264576.0000 - val_loss: 5264708096.0000 - val_mse: 5264708096.0000\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177590272.0000 - mse: 6177590272.0000 - val_loss: 5270865408.0000 - val_mse: 5270865408.0000\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178357248.0000 - mse: 6178357248.0000 - val_loss: 5266892288.0000 - val_mse: 5266892288.0000\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179272704.0000 - mse: 6179272704.0000 - val_loss: 5263197696.0000 - val_mse: 5263197696.0000\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179463680.0000 - mse: 6179463680.0000 - val_loss: 5262871552.0000 - val_mse: 5262871552.0000\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180166656.0000 - mse: 6180166656.0000 - val_loss: 5268698624.0000 - val_mse: 5268698624.0000\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178370560.0000 - mse: 6178370560.0000 - val_loss: 5269816320.0000 - val_mse: 5269816320.0000\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178582528.0000 - mse: 6178582528.0000 - val_loss: 5268982272.0000 - val_mse: 5268982272.0000\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179063808.0000 - mse: 6179063808.0000 - val_loss: 5266311680.0000 - val_mse: 5266311680.0000\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179280384.0000 - mse: 6179280384.0000 - val_loss: 5263835136.0000 - val_mse: 5263835136.0000\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6181128704.0000 - mse: 6181128704.0000 - val_loss: 5263359488.0000 - val_mse: 5263359488.0000\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179275264.0000 - mse: 6179275264.0000 - val_loss: 5265225216.0000 - val_mse: 5265225216.0000\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178245120.0000 - mse: 6178245120.0000 - val_loss: 5261012992.0000 - val_mse: 5261012992.0000\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177713664.0000 - mse: 6177713664.0000 - val_loss: 5261557760.0000 - val_mse: 5261557760.0000\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178838016.0000 - mse: 6178838016.0000 - val_loss: 5261324800.0000 - val_mse: 5261324800.0000\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179520000.0000 - mse: 6179520000.0000 - val_loss: 5260232704.0000 - val_mse: 5260232704.0000\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178821632.0000 - mse: 6178821632.0000 - val_loss: 5266005504.0000 - val_mse: 5266005504.0000\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180285952.0000 - mse: 6180285952.0000 - val_loss: 5270325760.0000 - val_mse: 5270325760.0000\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179274752.0000 - mse: 6179274752.0000 - val_loss: 5267147264.0000 - val_mse: 5267147264.0000\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179385856.0000 - mse: 6179385856.0000 - val_loss: 5269250048.0000 - val_mse: 5269250048.0000\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180084224.0000 - mse: 6180084224.0000 - val_loss: 5268201472.0000 - val_mse: 5268201472.0000\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178582528.0000 - mse: 6178582528.0000 - val_loss: 5268868608.0000 - val_mse: 5268868608.0000\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177945088.0000 - mse: 6177945088.0000 - val_loss: 5265814528.0000 - val_mse: 5265814528.0000\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178244096.0000 - mse: 6178244096.0000 - val_loss: 5270438400.0000 - val_mse: 5270438400.0000\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177556480.0000 - mse: 6177556480.0000 - val_loss: 5264547328.0000 - val_mse: 5264547328.0000\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178933760.0000 - mse: 6178933760.0000 - val_loss: 5266475008.0000 - val_mse: 5266475008.0000\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6181095936.0000 - mse: 6181095936.0000 - val_loss: 5269099520.0000 - val_mse: 5269099520.0000\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178850304.0000 - mse: 6178850304.0000 - val_loss: 5266594304.0000 - val_mse: 5266594304.0000\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180502528.0000 - mse: 6180502528.0000 - val_loss: 5266621952.0000 - val_mse: 5266621952.0000\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179309568.0000 - mse: 6179309568.0000 - val_loss: 5269400576.0000 - val_mse: 5269400576.0000\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 6180463104.0000 - mse: 6180463104.0000 - val_loss: 5266641408.0000 - val_mse: 5266641408.0000\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179255296.0000 - mse: 6179255296.0000 - val_loss: 5267978752.0000 - val_mse: 5267978752.0000\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178927616.0000 - mse: 6178927616.0000 - val_loss: 5263378432.0000 - val_mse: 5263378432.0000\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177921024.0000 - mse: 6177921024.0000 - val_loss: 5264165376.0000 - val_mse: 5264165376.0000\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179286016.0000 - mse: 6179286016.0000 - val_loss: 5263470592.0000 - val_mse: 5263470592.0000\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177683456.0000 - mse: 6177683456.0000 - val_loss: 5265701376.0000 - val_mse: 5265701376.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6176453632.0000 - mse: 6176453632.0000 - val_loss: 5260632576.0000 - val_mse: 5260632576.0000\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178854400.0000 - mse: 6178854400.0000 - val_loss: 5260834816.0000 - val_mse: 5260834816.0000\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177815040.0000 - mse: 6177815040.0000 - val_loss: 5261245952.0000 - val_mse: 5261245952.0000\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178037760.0000 - mse: 6178037760.0000 - val_loss: 5264544256.0000 - val_mse: 5264544256.0000\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177995264.0000 - mse: 6177995264.0000 - val_loss: 5266788864.0000 - val_mse: 5266788864.0000\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178298880.0000 - mse: 6178298880.0000 - val_loss: 5268889600.0000 - val_mse: 5268889600.0000\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180296704.0000 - mse: 6180296704.0000 - val_loss: 5272152064.0000 - val_mse: 5272152064.0000\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178166784.0000 - mse: 6178166784.0000 - val_loss: 5262718976.0000 - val_mse: 5262718976.0000\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178979840.0000 - mse: 6178979840.0000 - val_loss: 5264586240.0000 - val_mse: 5264586240.0000\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178937856.0000 - mse: 6178937856.0000 - val_loss: 5267518464.0000 - val_mse: 5267518464.0000\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180231680.0000 - mse: 6180231680.0000 - val_loss: 5269286400.0000 - val_mse: 5269286400.0000\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178934784.0000 - mse: 6178934784.0000 - val_loss: 5266675200.0000 - val_mse: 5266675200.0000\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177624064.0000 - mse: 6177624064.0000 - val_loss: 5264618496.0000 - val_mse: 5264618496.0000\n",
      "Epoch 63/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178160128.0000 - mse: 6178160128.0000 - val_loss: 5265838080.0000 - val_mse: 5265838080.0000\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178780672.0000 - mse: 6178780672.0000 - val_loss: 5267240448.0000 - val_mse: 5267240448.0000\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178353152.0000 - mse: 6178353152.0000 - val_loss: 5264676864.0000 - val_mse: 5264676864.0000\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177835008.0000 - mse: 6177835008.0000 - val_loss: 5267948032.0000 - val_mse: 5267948032.0000\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177532416.0000 - mse: 6177532416.0000 - val_loss: 5260449280.0000 - val_mse: 5260449280.0000\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178797056.0000 - mse: 6178797056.0000 - val_loss: 5263107584.0000 - val_mse: 5263107584.0000\n",
      "Epoch 69/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180521472.0000 - mse: 6180521472.0000 - val_loss: 5265624576.0000 - val_mse: 5265624576.0000\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177871360.0000 - mse: 6177871360.0000 - val_loss: 5267170304.0000 - val_mse: 5267170304.0000\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177853440.0000 - mse: 6177853440.0000 - val_loss: 5264233984.0000 - val_mse: 5264233984.0000\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177746944.0000 - mse: 6177746944.0000 - val_loss: 5271580160.0000 - val_mse: 5271580160.0000\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179560960.0000 - mse: 6179560960.0000 - val_loss: 5267512320.0000 - val_mse: 5267512320.0000\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177867264.0000 - mse: 6177867264.0000 - val_loss: 5260231680.0000 - val_mse: 5260231680.0000\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178613248.0000 - mse: 6178613248.0000 - val_loss: 5259386880.0000 - val_mse: 5259386880.0000\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179692032.0000 - mse: 6179692032.0000 - val_loss: 5260990464.0000 - val_mse: 5260990464.0000\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178144768.0000 - mse: 6178144768.0000 - val_loss: 5260498432.0000 - val_mse: 5260498432.0000\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177206784.0000 - mse: 6177206784.0000 - val_loss: 5260609536.0000 - val_mse: 5260609536.0000\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178675712.0000 - mse: 6178675712.0000 - val_loss: 5264525312.0000 - val_mse: 5264525312.0000\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177431552.0000 - mse: 6177431552.0000 - val_loss: 5265144832.0000 - val_mse: 5265144832.0000\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180695040.0000 - mse: 6180695040.0000 - val_loss: 5262731264.0000 - val_mse: 5262731264.0000\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179765760.0000 - mse: 6179765760.0000 - val_loss: 5263871488.0000 - val_mse: 5263871488.0000\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178373632.0000 - mse: 6178373632.0000 - val_loss: 5269455872.0000 - val_mse: 5269455872.0000\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179477504.0000 - mse: 6179477504.0000 - val_loss: 5270922752.0000 - val_mse: 5270922752.0000\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179456512.0000 - mse: 6179456512.0000 - val_loss: 5265811456.0000 - val_mse: 5265811456.0000\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179045888.0000 - mse: 6179045888.0000 - val_loss: 5266823168.0000 - val_mse: 5266823168.0000\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179522048.0000 - mse: 6179522048.0000 - val_loss: 5269013504.0000 - val_mse: 5269013504.0000\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177885184.0000 - mse: 6177885184.0000 - val_loss: 5270408192.0000 - val_mse: 5270408192.0000\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179228672.0000 - mse: 6179228672.0000 - val_loss: 5267466240.0000 - val_mse: 5267466240.0000\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179243520.0000 - mse: 6179243520.0000 - val_loss: 5270152704.0000 - val_mse: 5270152704.0000\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179931136.0000 - mse: 6179931136.0000 - val_loss: 5274131968.0000 - val_mse: 5274131968.0000\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178732032.0000 - mse: 6178732032.0000 - val_loss: 5267540480.0000 - val_mse: 5267540480.0000\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178785792.0000 - mse: 6178785792.0000 - val_loss: 5269238272.0000 - val_mse: 5269238272.0000\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177862144.0000 - mse: 6177862144.0000 - val_loss: 5263623168.0000 - val_mse: 5263623168.0000\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178370560.0000 - mse: 6178370560.0000 - val_loss: 5264280064.0000 - val_mse: 5264280064.0000\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178478080.0000 - mse: 6178478080.0000 - val_loss: 5267492352.0000 - val_mse: 5267492352.0000\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178152448.0000 - mse: 6178152448.0000 - val_loss: 5270254592.0000 - val_mse: 5270254592.0000\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177465344.0000 - mse: 6177465344.0000 - val_loss: 5273313792.0000 - val_mse: 5273313792.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179822592.0000 - mse: 6179822592.0000 - val_loss: 5273121280.0000 - val_mse: 5273121280.0000\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178236928.0000 - mse: 6178236928.0000 - val_loss: 5266297344.0000 - val_mse: 5266297344.0000\n",
      "Epoch 101/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177673728.0000 - mse: 6177673728.0000 - val_loss: 5263979520.0000 - val_mse: 5263979520.0000\n",
      "Epoch 102/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180683776.0000 - mse: 6180683776.0000 - val_loss: 5264816128.0000 - val_mse: 5264816128.0000\n",
      "Epoch 103/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177735680.0000 - mse: 6177735680.0000 - val_loss: 5267150336.0000 - val_mse: 5267150336.0000\n",
      "Epoch 104/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178285056.0000 - mse: 6178285056.0000 - val_loss: 5270343680.0000 - val_mse: 5270343680.0000\n",
      "Epoch 105/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179721728.0000 - mse: 6179721728.0000 - val_loss: 5268530688.0000 - val_mse: 5268530688.0000\n",
      "Epoch 106/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177932800.0000 - mse: 6177932800.0000 - val_loss: 5270512128.0000 - val_mse: 5270512128.0000\n",
      "Epoch 107/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179020800.0000 - mse: 6179020800.0000 - val_loss: 5273118208.0000 - val_mse: 5273118208.0000\n",
      "Epoch 108/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178751488.0000 - mse: 6178751488.0000 - val_loss: 5266158080.0000 - val_mse: 5266158080.0000\n",
      "Epoch 109/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178951168.0000 - mse: 6178951168.0000 - val_loss: 5263149568.0000 - val_mse: 5263149568.0000\n",
      "Epoch 110/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179544064.0000 - mse: 6179544064.0000 - val_loss: 5266425344.0000 - val_mse: 5266425344.0000\n",
      "Epoch 111/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178226176.0000 - mse: 6178226176.0000 - val_loss: 5269767680.0000 - val_mse: 5269767680.0000\n",
      "Epoch 112/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178733056.0000 - mse: 6178733056.0000 - val_loss: 5273294848.0000 - val_mse: 5273294848.0000\n",
      "Epoch 113/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178653696.0000 - mse: 6178653696.0000 - val_loss: 5269920256.0000 - val_mse: 5269920256.0000\n",
      "Epoch 114/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179049984.0000 - mse: 6179049984.0000 - val_loss: 5264845312.0000 - val_mse: 5264845312.0000\n",
      "Epoch 115/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178017792.0000 - mse: 6178017792.0000 - val_loss: 5267883008.0000 - val_mse: 5267883008.0000\n",
      "Epoch 116/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178718208.0000 - mse: 6178718208.0000 - val_loss: 5265258496.0000 - val_mse: 5265258496.0000\n",
      "Epoch 117/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179225088.0000 - mse: 6179225088.0000 - val_loss: 5263307264.0000 - val_mse: 5263307264.0000\n",
      "Epoch 118/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178190848.0000 - mse: 6178190848.0000 - val_loss: 5260078592.0000 - val_mse: 5260078592.0000\n",
      "Epoch 119/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180142080.0000 - mse: 6180142080.0000 - val_loss: 5259432960.0000 - val_mse: 5259432960.0000\n",
      "Epoch 120/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180303872.0000 - mse: 6180303872.0000 - val_loss: 5261032448.0000 - val_mse: 5261032448.0000\n",
      "Epoch 121/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179014656.0000 - mse: 6179014656.0000 - val_loss: 5266994176.0000 - val_mse: 5266994176.0000\n",
      "Epoch 122/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178760704.0000 - mse: 6178760704.0000 - val_loss: 5265846272.0000 - val_mse: 5265846272.0000\n",
      "Epoch 123/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179358720.0000 - mse: 6179358720.0000 - val_loss: 5268921856.0000 - val_mse: 5268921856.0000\n",
      "Epoch 124/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177680896.0000 - mse: 6177680896.0000 - val_loss: 5263378944.0000 - val_mse: 5263378944.0000\n",
      "Epoch 125/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178500608.0000 - mse: 6178500608.0000 - val_loss: 5263176704.0000 - val_mse: 5263176704.0000\n",
      "Epoch 126/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179350528.0000 - mse: 6179350528.0000 - val_loss: 5265279488.0000 - val_mse: 5265279488.0000\n",
      "Epoch 127/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179436544.0000 - mse: 6179436544.0000 - val_loss: 5268942848.0000 - val_mse: 5268942848.0000\n",
      "Epoch 128/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179389952.0000 - mse: 6179389952.0000 - val_loss: 5271616000.0000 - val_mse: 5271616000.0000\n",
      "Epoch 129/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178838528.0000 - mse: 6178838528.0000 - val_loss: 5271705600.0000 - val_mse: 5271705600.0000\n",
      "Epoch 130/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178886656.0000 - mse: 6178886656.0000 - val_loss: 5269447168.0000 - val_mse: 5269447168.0000\n",
      "Epoch 131/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178988544.0000 - mse: 6178988544.0000 - val_loss: 5266459136.0000 - val_mse: 5266459136.0000\n",
      "Epoch 132/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178448384.0000 - mse: 6178448384.0000 - val_loss: 5269647872.0000 - val_mse: 5269647872.0000\n",
      "Epoch 133/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178097152.0000 - mse: 6178097152.0000 - val_loss: 5268219904.0000 - val_mse: 5268219904.0000\n",
      "Epoch 134/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177782784.0000 - mse: 6177782784.0000 - val_loss: 5268884992.0000 - val_mse: 5268884992.0000\n",
      "Epoch 135/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180130304.0000 - mse: 6180130304.0000 - val_loss: 5269565952.0000 - val_mse: 5269565952.0000\n",
      "Epoch 136/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179798528.0000 - mse: 6179798528.0000 - val_loss: 5269430784.0000 - val_mse: 5269430784.0000\n",
      "Epoch 137/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179473408.0000 - mse: 6179473408.0000 - val_loss: 5270510592.0000 - val_mse: 5270510592.0000\n",
      "Epoch 138/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179349504.0000 - mse: 6179349504.0000 - val_loss: 5266538496.0000 - val_mse: 5266538496.0000\n",
      "Epoch 139/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179746816.0000 - mse: 6179746816.0000 - val_loss: 5269096448.0000 - val_mse: 5269096448.0000\n",
      "Epoch 140/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177895424.0000 - mse: 6177895424.0000 - val_loss: 5264762368.0000 - val_mse: 5264762368.0000\n",
      "Epoch 141/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6178215424.0000 - mse: 6178215424.0000 - val_loss: 5258152960.0000 - val_mse: 5258152960.0000\n",
      "Epoch 142/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179196416.0000 - mse: 6179196416.0000 - val_loss: 5263807488.0000 - val_mse: 5263807488.0000\n",
      "Epoch 143/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179031552.0000 - mse: 6179031552.0000 - val_loss: 5267237376.0000 - val_mse: 5267237376.0000\n",
      "Epoch 144/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177322496.0000 - mse: 6177322496.0000 - val_loss: 5261093376.0000 - val_mse: 5261093376.0000\n",
      "Epoch 145/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179624960.0000 - mse: 6179624960.0000 - val_loss: 5265255936.0000 - val_mse: 5265255936.0000\n",
      "Epoch 146/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6180126720.0000 - mse: 6180126720.0000 - val_loss: 5266672128.0000 - val_mse: 5266672128.0000\n",
      "Epoch 147/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: 6179392512.0000 - mse: 6179392512.0000 - val_loss: 5271454208.0000 - val_mse: 5271454208.0000\n",
      "Epoch 148/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179410432.0000 - mse: 6179410432.0000 - val_loss: 5268504064.0000 - val_mse: 5268504064.0000\n",
      "Epoch 149/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6179281920.0000 - mse: 6179281920.0000 - val_loss: 5266604544.0000 - val_mse: 5266604544.0000\n",
      "Epoch 150/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6177880064.0000 - mse: 6177880064.0000 - val_loss: 5266082304.0000 - val_mse: 5266082304.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "normalized_val = normalized_input_model.fit(X_train, y_train,\n",
    "                          batch_size=32, epochs=150,\n",
    "                          validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f277c29d850>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(normalized_val.history['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _**Note that you still haven't achieved convergence! From here, it's time to normalize the output data.**_\n",
    "\n",
    "## Normalizing the output\n",
    "\n",
    "Again, use `StandardScaler()` to: \n",
    "\n",
    "- Fit and transform `y_train` \n",
    "- Transform `y_val` and `y_test` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate StandardScaler\n",
    "ss_y = StandardScaler()\n",
    "\n",
    "# Fit and transform train labels\n",
    "y_train_scaled = ss_y.fit_transform(y_train)\n",
    "\n",
    "# Transform validate and test labels\n",
    "y_val_scaled = ss_y.transform(y_val)\n",
    "y_test_scaled = ss_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below: \n",
    "- Train the `normalized_model` on normalized input (`X_train`) and output (`y_train_scaled`) \n",
    "- Set a batch size of 32 and train for 150 epochs \n",
    "- Specify the `validation_data` as `(X_val, y_val_scaled)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5305 - mse: 0.5305 - val_loss: 0.2076 - val_mse: 0.2076\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2372 - mse: 0.2372 - val_loss: 0.1710 - val_mse: 0.1710\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1958 - mse: 0.1958 - val_loss: 0.1414 - val_mse: 0.1414\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1686 - mse: 0.1686 - val_loss: 0.1392 - val_mse: 0.1392\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1529 - mse: 0.1529 - val_loss: 0.1632 - val_mse: 0.1632\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1420 - mse: 0.1420 - val_loss: 0.1327 - val_mse: 0.1327\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1293 - mse: 0.1293 - val_loss: 0.1452 - val_mse: 0.1452\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1185 - mse: 0.1185 - val_loss: 0.1400 - val_mse: 0.1400\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1131 - mse: 0.1131 - val_loss: 0.1403 - val_mse: 0.1403\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1064 - mse: 0.1064 - val_loss: 0.1363 - val_mse: 0.1363\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1011 - mse: 0.1011 - val_loss: 0.1291 - val_mse: 0.1291\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.1302 - val_mse: 0.1302\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0919 - mse: 0.0919 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0864 - mse: 0.0864 - val_loss: 0.1316 - val_mse: 0.1316\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0842 - mse: 0.0842 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0780 - mse: 0.0780 - val_loss: 0.1369 - val_mse: 0.1369\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0760 - mse: 0.0760 - val_loss: 0.1321 - val_mse: 0.1321\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0727 - mse: 0.0727 - val_loss: 0.1400 - val_mse: 0.1400\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0703 - mse: 0.0703 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0691 - mse: 0.0691 - val_loss: 0.1319 - val_mse: 0.1319\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0647 - mse: 0.0647 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0622 - mse: 0.0622 - val_loss: 0.1361 - val_mse: 0.1361\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0601 - mse: 0.0601 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0575 - mse: 0.0575 - val_loss: 0.1359 - val_mse: 0.1359\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0578 - mse: 0.0578 - val_loss: 0.1344 - val_mse: 0.1344\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0543 - mse: 0.0543 - val_loss: 0.1411 - val_mse: 0.1411\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0530 - mse: 0.0530 - val_loss: 0.1405 - val_mse: 0.1405\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0514 - mse: 0.0514 - val_loss: 0.1361 - val_mse: 0.1361\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.1343 - val_mse: 0.1343\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0489 - mse: 0.0489 - val_loss: 0.1333 - val_mse: 0.1333\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0463 - mse: 0.0463 - val_loss: 0.1330 - val_mse: 0.1330\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0461 - mse: 0.0461 - val_loss: 0.1304 - val_mse: 0.1304\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0449 - mse: 0.0449 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0440 - mse: 0.0440 - val_loss: 0.1340 - val_mse: 0.1340\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0431 - mse: 0.0431 - val_loss: 0.1372 - val_mse: 0.1372\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0419 - mse: 0.0419 - val_loss: 0.1325 - val_mse: 0.1325\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0410 - mse: 0.0410 - val_loss: 0.1324 - val_mse: 0.1324\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0400 - mse: 0.0400 - val_loss: 0.1334 - val_mse: 0.1334\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0393 - mse: 0.0393 - val_loss: 0.1313 - val_mse: 0.1313\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0382 - mse: 0.0382 - val_loss: 0.1328 - val_mse: 0.1328\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.1348 - val_mse: 0.1348\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0368 - mse: 0.0368 - val_loss: 0.1294 - val_mse: 0.1294\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.1306 - val_mse: 0.1306\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0354 - mse: 0.0354 - val_loss: 0.1324 - val_mse: 0.1324\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0343 - mse: 0.0343 - val_loss: 0.1295 - val_mse: 0.1295\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0336 - mse: 0.0336 - val_loss: 0.1315 - val_mse: 0.1315\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0332 - mse: 0.0332 - val_loss: 0.1292 - val_mse: 0.1292\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.1310 - val_mse: 0.1310\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0319 - mse: 0.0319 - val_loss: 0.1329 - val_mse: 0.1329\n",
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0310 - mse: 0.0310 - val_loss: 0.1311 - val_mse: 0.1311\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0304 - mse: 0.0304 - val_loss: 0.1292 - val_mse: 0.1292\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0299 - mse: 0.0299 - val_loss: 0.1298 - val_mse: 0.1298\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1292 - val_mse: 0.1292\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0290 - mse: 0.0290 - val_loss: 0.1284 - val_mse: 0.1284\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.1303 - val_mse: 0.1303\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0283 - mse: 0.0283 - val_loss: 0.1275 - val_mse: 0.1275\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0275 - mse: 0.0275 - val_loss: 0.1274 - val_mse: 0.1274\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0272 - mse: 0.0272 - val_loss: 0.1296 - val_mse: 0.1296\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.1295 - val_mse: 0.1295\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.1290 - val_mse: 0.1290\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.1293 - val_mse: 0.1293\n",
      "Epoch 63/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1286 - val_mse: 0.1286\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.1289 - val_mse: 0.1289\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.1329 - val_mse: 0.1329\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.1350 - val_mse: 0.1350\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0244 - mse: 0.0244 - val_loss: 0.1321 - val_mse: 0.1321\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.1323 - val_mse: 0.1323\n",
      "Epoch 69/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0230 - mse: 0.0230 - val_loss: 0.1305 - val_mse: 0.1305\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.1299 - val_mse: 0.1299\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.1291 - val_mse: 0.1291\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.1310 - val_mse: 0.1310\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.1303 - val_mse: 0.1303\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.1299 - val_mse: 0.1299\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0210 - mse: 0.0210 - val_loss: 0.1308 - val_mse: 0.1308\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.1341 - val_mse: 0.1341\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.1310 - val_mse: 0.1310\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.1316 - val_mse: 0.1316\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.1305 - val_mse: 0.1305\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0198 - mse: 0.0198 - val_loss: 0.1300 - val_mse: 0.1300\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.1323 - val_mse: 0.1323\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.1307 - val_mse: 0.1307\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.1338 - val_mse: 0.1338\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.1318 - val_mse: 0.1318\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.1314 - val_mse: 0.1314\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.1309 - val_mse: 0.1309\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.1329 - val_mse: 0.1329\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.1335 - val_mse: 0.1335\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.1317 - val_mse: 0.1317\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.1309 - val_mse: 0.1309\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.1328 - val_mse: 0.1328\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0172 - mse: 0.0172 - val_loss: 0.1327 - val_mse: 0.1327\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0171 - mse: 0.0171 - val_loss: 0.1313 - val_mse: 0.1313\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.1336 - val_mse: 0.1336\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.1343 - val_mse: 0.1343\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.1347 - val_mse: 0.1347\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.1331 - val_mse: 0.1331\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.1319 - val_mse: 0.1319\n",
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.1333 - val_mse: 0.1333\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.1321 - val_mse: 0.1321\n",
      "Epoch 101/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.1349 - val_mse: 0.1349\n",
      "Epoch 102/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.1331 - val_mse: 0.1331\n",
      "Epoch 103/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.1344 - val_mse: 0.1344\n",
      "Epoch 104/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0151 - mse: 0.0151 - val_loss: 0.1349 - val_mse: 0.1349\n",
      "Epoch 105/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.1349 - val_mse: 0.1349\n",
      "Epoch 106/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0148 - mse: 0.0148 - val_loss: 0.1351 - val_mse: 0.1351\n",
      "Epoch 107/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.1352 - val_mse: 0.1352\n",
      "Epoch 108/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.1326 - val_mse: 0.1326\n",
      "Epoch 109/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.1427 - val_mse: 0.1427\n",
      "Epoch 110/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.1340 - val_mse: 0.1340\n",
      "Epoch 111/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0139 - mse: 0.0139 - val_loss: 0.1362 - val_mse: 0.1362\n",
      "Epoch 112/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.1375 - val_mse: 0.1375\n",
      "Epoch 113/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.1351 - val_mse: 0.1351\n",
      "Epoch 114/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.1385 - val_mse: 0.1385\n",
      "Epoch 115/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.1359 - val_mse: 0.1359\n",
      "Epoch 116/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 117/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.1373 - val_mse: 0.1373\n",
      "Epoch 118/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.1373 - val_mse: 0.1373\n",
      "Epoch 119/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 120/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.1360 - val_mse: 0.1360\n",
      "Epoch 121/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.1349 - val_mse: 0.1349\n",
      "Epoch 122/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.1370 - val_mse: 0.1370\n",
      "Epoch 123/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 124/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.1361 - val_mse: 0.1361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.1370 - val_mse: 0.1370\n",
      "Epoch 126/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.1363 - val_mse: 0.1363\n",
      "Epoch 127/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.1377 - val_mse: 0.1377\n",
      "Epoch 128/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.1360 - val_mse: 0.1360\n",
      "Epoch 129/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 130/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.1383 - val_mse: 0.1383\n",
      "Epoch 131/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 132/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.1370 - val_mse: 0.1370\n",
      "Epoch 133/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 134/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.1428 - val_mse: 0.1428\n",
      "Epoch 135/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.1389 - val_mse: 0.1389\n",
      "Epoch 136/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.1383 - val_mse: 0.1383\n",
      "Epoch 137/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 138/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 139/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.1380 - val_mse: 0.1380\n",
      "Epoch 140/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.1376 - val_mse: 0.1376\n",
      "Epoch 141/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.1393 - val_mse: 0.1393\n",
      "Epoch 142/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.1416 - val_mse: 0.1416\n",
      "Epoch 143/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0102 - mse: 0.0102 - val_loss: 0.1372 - val_mse: 0.1372\n",
      "Epoch 144/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 145/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.1380 - val_mse: 0.1380\n",
      "Epoch 146/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1390 - val_mse: 0.1390\n",
      "Epoch 147/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1393 - val_mse: 0.1393\n",
      "Epoch 148/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.1413 - val_mse: 0.1413\n",
      "Epoch 149/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 150/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.1405 - val_mse: 0.1405\n"
     ]
    }
   ],
   "source": [
    "# Model with all normalized inputs and outputs\n",
    "np.random.seed(123)\n",
    "normalized_model = Sequential()\n",
    "normalized_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "normalized_model.add(layers.Dense(50, activation='relu'))\n",
    "normalized_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "normalized_model.compile(optimizer='SGD', \n",
    "                         loss='mse', \n",
    "                         metrics=['mse']) \n",
    "\n",
    "# Train the model\n",
    "normalized_val = normalized_model.fit(X_train, y_train_scaled,\n",
    "                    batch_size=32, epochs=150,\n",
    "                    validation_data=(X_val, y_val_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f269c2e1dc0>]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(normalized_val.history['mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done! After normalizing both the input and output, the model finally converged. \n",
    "\n",
    "- Evaluate the model (`normalized_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 485us/step - loss: 0.0067 - mse: 0.0067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.006652175914496183, 0.006652175914496183]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "normalized_model.evaluate(X_train, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate the model (`normalized_model`) on validate data (`X_val` and `y_val_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 574us/step - loss: 0.1659 - mse: 0.1659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1658889353275299, 0.1658889353275299]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on validate data\n",
    "normalized_model.evaluate(X_val, y_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output is normalized, the metric above is not interpretable. To remedy this: \n",
    "\n",
    "- Generate predictions on validate data (`X_val`) \n",
    "- Transform these predictions back to original scale using `ss_y` \n",
    "- Now you can calculate the RMSE in the original units with `y_val` and `y_val_pred` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32006.650282021314"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions on validate data\n",
    "y_val_pred_scaled = normalized_model.predict(X_val)\n",
    "\n",
    "# Transform the predictions back to original scale\n",
    "y_val_pred = ss_y.inverse_transform(y_val_pred_scaled)\n",
    "\n",
    "# RMSE of validate data\n",
    "mean_squared_error(y_val, y_val_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that you have a converged model, you can also experiment with alternative optimizers and initialization strategies to see if you can find a better global minimum. (After all, the current models may have converged to a local minimum.) \n",
    "\n",
    "## Using Weight Initializers\n",
    "\n",
    "In this section you will to use alternative initialization and optimization strategies. At the end, you'll then be asked to select the model which you believe performs the best.  \n",
    "\n",
    "##  He Initialization\n",
    "\n",
    "In the cell below, sepcify the following in the first hidden layer:  \n",
    "  - 100 units \n",
    "  - `'relu'` activation \n",
    "  - `input_shape` \n",
    "  - `kernel_initializer='he_normal'`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.5173 - mse: 0.5173 - val_loss: 0.2268 - val_mse: 0.2268\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2477 - mse: 0.2477 - val_loss: 0.1652 - val_mse: 0.1652\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2105 - mse: 0.2105 - val_loss: 0.1462 - val_mse: 0.1462\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1695 - mse: 0.1695 - val_loss: 0.1402 - val_mse: 0.1402\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1512 - mse: 0.1512 - val_loss: 0.1491 - val_mse: 0.1491\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1361 - mse: 0.1361 - val_loss: 0.1327 - val_mse: 0.1327\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1257 - mse: 0.1257 - val_loss: 0.1295 - val_mse: 0.1295\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1177 - mse: 0.1177 - val_loss: 0.1470 - val_mse: 0.1470\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1153 - mse: 0.1153 - val_loss: 0.1269 - val_mse: 0.1269\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1058 - mse: 0.1058 - val_loss: 0.1289 - val_mse: 0.1289\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1049 - mse: 0.1049 - val_loss: 0.1294 - val_mse: 0.1294\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0944 - mse: 0.0944 - val_loss: 0.1470 - val_mse: 0.1470\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0936 - mse: 0.0936 - val_loss: 0.1275 - val_mse: 0.1275\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0880 - mse: 0.0880 - val_loss: 0.1305 - val_mse: 0.1305\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0843 - mse: 0.0843 - val_loss: 0.1274 - val_mse: 0.1274\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0827 - mse: 0.0827 - val_loss: 0.1316 - val_mse: 0.1316\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0787 - mse: 0.0787 - val_loss: 0.1314 - val_mse: 0.1314\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0772 - mse: 0.0772 - val_loss: 0.1285 - val_mse: 0.1285\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0741 - mse: 0.0741 - val_loss: 0.1302 - val_mse: 0.1302\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0714 - mse: 0.0714 - val_loss: 0.1279 - val_mse: 0.1279\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0679 - mse: 0.0679 - val_loss: 0.1280 - val_mse: 0.1280\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0672 - mse: 0.0672 - val_loss: 0.1312 - val_mse: 0.1312\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0645 - mse: 0.0645 - val_loss: 0.1294 - val_mse: 0.1294\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0616 - mse: 0.0616 - val_loss: 0.1333 - val_mse: 0.1333\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0614 - mse: 0.0614 - val_loss: 0.1291 - val_mse: 0.1291\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0585 - mse: 0.0585 - val_loss: 0.1358 - val_mse: 0.1358\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0576 - mse: 0.0576 - val_loss: 0.1316 - val_mse: 0.1316\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0552 - mse: 0.0552 - val_loss: 0.1304 - val_mse: 0.1304\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0536 - mse: 0.0536 - val_loss: 0.1296 - val_mse: 0.1296\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0535 - mse: 0.0535 - val_loss: 0.1308 - val_mse: 0.1308\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0514 - mse: 0.0514 - val_loss: 0.1319 - val_mse: 0.1319\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0507 - mse: 0.0507 - val_loss: 0.1338 - val_mse: 0.1338\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0487 - mse: 0.0487 - val_loss: 0.1336 - val_mse: 0.1336\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0473 - mse: 0.0473 - val_loss: 0.1326 - val_mse: 0.1326\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0470 - mse: 0.0470 - val_loss: 0.1361 - val_mse: 0.1361\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0459 - mse: 0.0459 - val_loss: 0.1346 - val_mse: 0.1346\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0444 - mse: 0.0444 - val_loss: 0.1347 - val_mse: 0.1347\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0445 - mse: 0.0445 - val_loss: 0.1343 - val_mse: 0.1343\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0421 - mse: 0.0421 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0418 - mse: 0.0418 - val_loss: 0.1358 - val_mse: 0.1358\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0405 - mse: 0.0405 - val_loss: 0.1352 - val_mse: 0.1352\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0394 - mse: 0.0394 - val_loss: 0.1382 - val_mse: 0.1382\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0389 - mse: 0.0389 - val_loss: 0.1380 - val_mse: 0.1380\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0376 - mse: 0.0376 - val_loss: 0.1364 - val_mse: 0.1364\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0371 - mse: 0.0371 - val_loss: 0.1390 - val_mse: 0.1390\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0365 - mse: 0.0365 - val_loss: 0.1376 - val_mse: 0.1376\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0362 - mse: 0.0362 - val_loss: 0.1383 - val_mse: 0.1383\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0346 - mse: 0.0346 - val_loss: 0.1439 - val_mse: 0.1439\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0344 - mse: 0.0344 - val_loss: 0.1362 - val_mse: 0.1362\n",
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.1384 - val_mse: 0.1384\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0338 - mse: 0.0338 - val_loss: 0.1419 - val_mse: 0.1419\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0326 - mse: 0.0326 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0328 - mse: 0.0328 - val_loss: 0.1375 - val_mse: 0.1375\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0320 - mse: 0.0320 - val_loss: 0.1403 - val_mse: 0.1403\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0312 - mse: 0.0312 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0305 - mse: 0.0305 - val_loss: 0.1413 - val_mse: 0.1413\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0300 - mse: 0.0300 - val_loss: 0.1398 - val_mse: 0.1398\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0295 - mse: 0.0295 - val_loss: 0.1392 - val_mse: 0.1392\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.1388 - val_mse: 0.1388\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0285 - mse: 0.0285 - val_loss: 0.1378 - val_mse: 0.1378\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1400 - val_mse: 0.1400\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0279 - mse: 0.0279 - val_loss: 0.1403 - val_mse: 0.1403\n",
      "Epoch 63/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0271 - mse: 0.0271 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.1395 - val_mse: 0.1395\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0263 - mse: 0.0263 - val_loss: 0.1400 - val_mse: 0.1400\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.1403 - val_mse: 0.1403\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.1397 - val_mse: 0.1397\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.1407 - val_mse: 0.1407\n",
      "Epoch 69/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.1434 - val_mse: 0.1434\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.1400 - val_mse: 0.1400\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0241 - mse: 0.0241 - val_loss: 0.1415 - val_mse: 0.1415\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0240 - mse: 0.0240 - val_loss: 0.1403 - val_mse: 0.1403\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0232 - mse: 0.0232 - val_loss: 0.1421 - val_mse: 0.1421\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0234 - mse: 0.0234 - val_loss: 0.1418 - val_mse: 0.1418\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.1419 - val_mse: 0.1419\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0225 - mse: 0.0225 - val_loss: 0.1394 - val_mse: 0.1394\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.1403 - val_mse: 0.1403\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.1426 - val_mse: 0.1426\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0214 - mse: 0.0214 - val_loss: 0.1418 - val_mse: 0.1418\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.1441 - val_mse: 0.1441\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0216 - mse: 0.0216 - val_loss: 0.1424 - val_mse: 0.1424\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0208 - mse: 0.0208 - val_loss: 0.1442 - val_mse: 0.1442\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.1448 - val_mse: 0.1448\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.1427 - val_mse: 0.1427\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0206 - mse: 0.0206 - val_loss: 0.1439 - val_mse: 0.1439\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.1424 - val_mse: 0.1424\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.1425 - val_mse: 0.1425\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0192 - mse: 0.0192 - val_loss: 0.1434 - val_mse: 0.1434\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0193 - mse: 0.0193 - val_loss: 0.1448 - val_mse: 0.1448\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.1435 - val_mse: 0.1435\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.1426 - val_mse: 0.1426\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.1451 - val_mse: 0.1451\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.1449 - val_mse: 0.1449\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.1436 - val_mse: 0.1436\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.1446 - val_mse: 0.1446\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.1436 - val_mse: 0.1436\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.1457 - val_mse: 0.1457\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.1445 - val_mse: 0.1445\n",
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.1445 - val_mse: 0.1445\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.1463 - val_mse: 0.1463\n",
      "Epoch 101/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.1431 - val_mse: 0.1431\n",
      "Epoch 102/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0165 - mse: 0.0165 - val_loss: 0.1441 - val_mse: 0.1441\n",
      "Epoch 103/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0162 - mse: 0.0162 - val_loss: 0.1448 - val_mse: 0.1448\n",
      "Epoch 104/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.1458 - val_mse: 0.1458\n",
      "Epoch 105/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.1444 - val_mse: 0.1444\n",
      "Epoch 106/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0156 - mse: 0.0156 - val_loss: 0.1462 - val_mse: 0.1462\n",
      "Epoch 107/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.1448 - val_mse: 0.1448\n",
      "Epoch 108/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0153 - mse: 0.0153 - val_loss: 0.1453 - val_mse: 0.1453\n",
      "Epoch 109/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.1463 - val_mse: 0.1463\n",
      "Epoch 110/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.1464 - val_mse: 0.1464\n",
      "Epoch 111/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.1454 - val_mse: 0.1454\n",
      "Epoch 112/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.1461 - val_mse: 0.1461\n",
      "Epoch 113/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0141 - mse: 0.0141 - val_loss: 0.1450 - val_mse: 0.1450\n",
      "Epoch 114/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0146 - mse: 0.0146 - val_loss: 0.1464 - val_mse: 0.1464\n",
      "Epoch 115/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.1478 - val_mse: 0.1478\n",
      "Epoch 116/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.1477 - val_mse: 0.1477\n",
      "Epoch 117/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.1474 - val_mse: 0.1474\n",
      "Epoch 118/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.1463 - val_mse: 0.1463\n",
      "Epoch 119/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0135 - mse: 0.0135 - val_loss: 0.1470 - val_mse: 0.1470\n",
      "Epoch 120/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.1468 - val_mse: 0.1468\n",
      "Epoch 121/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.1464 - val_mse: 0.1464\n",
      "Epoch 122/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.1472 - val_mse: 0.1472\n",
      "Epoch 123/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.1469 - val_mse: 0.1469\n",
      "Epoch 124/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.1460 - val_mse: 0.1460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0128 - mse: 0.0128 - val_loss: 0.1497 - val_mse: 0.1497\n",
      "Epoch 126/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.1494 - val_mse: 0.1494\n",
      "Epoch 127/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.1477 - val_mse: 0.1477\n",
      "Epoch 128/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.1468 - val_mse: 0.1468\n",
      "Epoch 129/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.1477 - val_mse: 0.1477\n",
      "Epoch 130/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.1473 - val_mse: 0.1473\n",
      "Epoch 131/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.1489 - val_mse: 0.1489\n",
      "Epoch 132/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.1474 - val_mse: 0.1474\n",
      "Epoch 133/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0117 - mse: 0.0117 - val_loss: 0.1491 - val_mse: 0.1491\n",
      "Epoch 134/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.1496 - val_mse: 0.1496\n",
      "Epoch 135/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.1479 - val_mse: 0.1479\n",
      "Epoch 136/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.1488 - val_mse: 0.1488\n",
      "Epoch 137/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.1494 - val_mse: 0.1494\n",
      "Epoch 138/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.1477 - val_mse: 0.1477\n",
      "Epoch 139/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.1489 - val_mse: 0.1489\n",
      "Epoch 140/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.1497 - val_mse: 0.1497\n",
      "Epoch 141/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.1493 - val_mse: 0.1493\n",
      "Epoch 142/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.1486 - val_mse: 0.1486\n",
      "Epoch 143/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.1505 - val_mse: 0.1505\n",
      "Epoch 144/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.1493 - val_mse: 0.1493\n",
      "Epoch 145/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.1499 - val_mse: 0.1499\n",
      "Epoch 146/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.1509 - val_mse: 0.1509\n",
      "Epoch 147/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1500 - val_mse: 0.1500\n",
      "Epoch 148/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1495 - val_mse: 0.1495\n",
      "Epoch 149/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1511 - val_mse: 0.1511\n",
      "Epoch 150/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.1510 - val_mse: 0.1510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f279c14f9a0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "he_model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "he_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,), kernel_initializer='he_normal'))\n",
    "\n",
    "# Add another hidden layer\n",
    "he_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "he_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "he_model.compile(optimizer='SGD', \n",
    "                 loss='mse', \n",
    "                 metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "he_model.fit(X_train, \n",
    "             y_train_scaled, \n",
    "             batch_size=32, \n",
    "             epochs=150, \n",
    "             validation_data=(X_val, y_val_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`he_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 487us/step - loss: 0.0092 - mse: 0.0092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00923660397529602, 0.00923660397529602]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "he_model.evaluate(X_train, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`he_model`) on validate data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 546us/step - loss: 0.1510 - mse: 0.1510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15097695589065552, 0.15097695589065552]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on validate data\n",
    "he_model.evaluate(X_val, y_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecun Initialization \n",
    "\n",
    "In the cell below, sepcify the following in the first hidden layer:  \n",
    "  - 100 units \n",
    "  - `'relu'` activation \n",
    "  - `input_shape` \n",
    "  - `kernel_initializer='lecun_normal'` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.2250 - val_mse: 0.2250\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2216 - mse: 0.2216 - val_loss: 0.1630 - val_mse: 0.1630\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1891 - mse: 0.1891 - val_loss: 0.1378 - val_mse: 0.1378\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1643 - mse: 0.1643 - val_loss: 0.1317 - val_mse: 0.1317\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1421 - mse: 0.1421 - val_loss: 0.1235 - val_mse: 0.1235\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1318 - mse: 0.1318 - val_loss: 0.1338 - val_mse: 0.1338\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1190 - mse: 0.1190 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1106 - mse: 0.1106 - val_loss: 0.1292 - val_mse: 0.1292\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1038 - mse: 0.1038 - val_loss: 0.1246 - val_mse: 0.1246\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0962 - mse: 0.0962 - val_loss: 0.1189 - val_mse: 0.1189\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0901 - mse: 0.0901 - val_loss: 0.1227 - val_mse: 0.1227\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0846 - mse: 0.0846 - val_loss: 0.1216 - val_mse: 0.1216\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0798 - mse: 0.0798 - val_loss: 0.1241 - val_mse: 0.1241\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0761 - mse: 0.0761 - val_loss: 0.1272 - val_mse: 0.1272\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0727 - mse: 0.0727 - val_loss: 0.1263 - val_mse: 0.1263\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0687 - mse: 0.0687 - val_loss: 0.1256 - val_mse: 0.1256\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0649 - mse: 0.0649 - val_loss: 0.1289 - val_mse: 0.1289\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0615 - mse: 0.0615 - val_loss: 0.1299 - val_mse: 0.1299\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0595 - mse: 0.0595 - val_loss: 0.1314 - val_mse: 0.1314\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0567 - mse: 0.0567 - val_loss: 0.1297 - val_mse: 0.1297\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0539 - mse: 0.0539 - val_loss: 0.1329 - val_mse: 0.1329\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0536 - mse: 0.0536 - val_loss: 0.1293 - val_mse: 0.1293\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0513 - mse: 0.0513 - val_loss: 0.1308 - val_mse: 0.1308\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0512 - mse: 0.0512 - val_loss: 0.1304 - val_mse: 0.1304\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0481 - mse: 0.0481 - val_loss: 0.1340 - val_mse: 0.1340\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0478 - mse: 0.0478 - val_loss: 0.1329 - val_mse: 0.1329\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0451 - mse: 0.0451 - val_loss: 0.1339 - val_mse: 0.1339\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.1310 - val_mse: 0.1310\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0433 - mse: 0.0433 - val_loss: 0.1342 - val_mse: 0.1342\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0417 - mse: 0.0417 - val_loss: 0.1310 - val_mse: 0.1310\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0407 - mse: 0.0407 - val_loss: 0.1382 - val_mse: 0.1382\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.1333 - val_mse: 0.1333\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0391 - mse: 0.0391 - val_loss: 0.1355 - val_mse: 0.1355\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0379 - mse: 0.0379 - val_loss: 0.1349 - val_mse: 0.1349\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0356 - mse: 0.0356 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.1392 - val_mse: 0.1392\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0353 - mse: 0.0353 - val_loss: 0.1439 - val_mse: 0.1439\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0347 - mse: 0.0347 - val_loss: 0.1390 - val_mse: 0.1390\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0337 - mse: 0.0337 - val_loss: 0.1381 - val_mse: 0.1381\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0330 - mse: 0.0330 - val_loss: 0.1396 - val_mse: 0.1396\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.1382 - val_mse: 0.1382\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0314 - mse: 0.0314 - val_loss: 0.1373 - val_mse: 0.1373\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0303 - mse: 0.0303 - val_loss: 0.1352 - val_mse: 0.1352\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0298 - mse: 0.0298 - val_loss: 0.1354 - val_mse: 0.1354\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.1370 - val_mse: 0.1370\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0296 - mse: 0.0296 - val_loss: 0.1385 - val_mse: 0.1385\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0282 - mse: 0.0282 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0280 - mse: 0.0280 - val_loss: 0.1377 - val_mse: 0.1377\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0273 - mse: 0.0273 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0268 - mse: 0.0268 - val_loss: 0.1392 - val_mse: 0.1392\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0261 - mse: 0.0261 - val_loss: 0.1385 - val_mse: 0.1385\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0266 - mse: 0.0266 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.1376 - val_mse: 0.1376\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.1377 - val_mse: 0.1377\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0236 - mse: 0.0236 - val_loss: 0.1388 - val_mse: 0.1388\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0237 - mse: 0.0237 - val_loss: 0.1375 - val_mse: 0.1375\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.1370 - val_mse: 0.1370\n",
      "Epoch 63/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0227 - mse: 0.0227 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0226 - mse: 0.0226 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0223 - mse: 0.0223 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0222 - mse: 0.0222 - val_loss: 0.1358 - val_mse: 0.1358\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0218 - mse: 0.0218 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 69/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0211 - mse: 0.0211 - val_loss: 0.1360 - val_mse: 0.1360\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0203 - mse: 0.0203 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0202 - mse: 0.0202 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.1385 - val_mse: 0.1385\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0199 - mse: 0.0199 - val_loss: 0.1366 - val_mse: 0.1366\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0189 - mse: 0.0189 - val_loss: 0.1391 - val_mse: 0.1391\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.1419 - val_mse: 0.1419\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0188 - mse: 0.0188 - val_loss: 0.1358 - val_mse: 0.1358\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.1369 - val_mse: 0.1369\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.1350 - val_mse: 0.1350\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.1368 - val_mse: 0.1368\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.1353 - val_mse: 0.1353\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.1370 - val_mse: 0.1370\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.1346 - val_mse: 0.1346\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0166 - mse: 0.0166 - val_loss: 0.1356 - val_mse: 0.1356\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0164 - mse: 0.0164 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0160 - mse: 0.0160 - val_loss: 0.1379 - val_mse: 0.1379\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0154 - mse: 0.0154 - val_loss: 0.1364 - val_mse: 0.1364\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0155 - mse: 0.0155 - val_loss: 0.1353 - val_mse: 0.1353\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0150 - mse: 0.0150 - val_loss: 0.1387 - val_mse: 0.1387\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.1355 - val_mse: 0.1355\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.1358 - val_mse: 0.1358\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.1373 - val_mse: 0.1373\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0145 - mse: 0.0145 - val_loss: 0.1361 - val_mse: 0.1361\n",
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0144 - mse: 0.0144 - val_loss: 0.1361 - val_mse: 0.1361\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0142 - mse: 0.0142 - val_loss: 0.1355 - val_mse: 0.1355\n",
      "Epoch 101/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.1360 - val_mse: 0.1360\n",
      "Epoch 102/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.1377 - val_mse: 0.1377\n",
      "Epoch 103/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0140 - mse: 0.0140 - val_loss: 0.1362 - val_mse: 0.1362\n",
      "Epoch 104/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 105/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 106/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.1362 - val_mse: 0.1362\n",
      "Epoch 107/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0130 - mse: 0.0130 - val_loss: 0.1367 - val_mse: 0.1367\n",
      "Epoch 108/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0136 - mse: 0.0136 - val_loss: 0.1355 - val_mse: 0.1355\n",
      "Epoch 109/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0132 - mse: 0.0132 - val_loss: 0.1356 - val_mse: 0.1356\n",
      "Epoch 110/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0124 - mse: 0.0124 - val_loss: 0.1361 - val_mse: 0.1361\n",
      "Epoch 111/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0125 - mse: 0.0125 - val_loss: 0.1388 - val_mse: 0.1388\n",
      "Epoch 112/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.1369 - val_mse: 0.1369\n",
      "Epoch 113/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.1352 - val_mse: 0.1352\n",
      "Epoch 114/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.1353 - val_mse: 0.1353\n",
      "Epoch 115/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.1361 - val_mse: 0.1361\n",
      "Epoch 116/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0118 - mse: 0.0118 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 117/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.1359 - val_mse: 0.1359\n",
      "Epoch 118/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0116 - mse: 0.0116 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 119/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.1371 - val_mse: 0.1371\n",
      "Epoch 120/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0113 - mse: 0.0113 - val_loss: 0.1363 - val_mse: 0.1363\n",
      "Epoch 121/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0112 - mse: 0.0112 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 122/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.1376 - val_mse: 0.1376\n",
      "Epoch 123/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.1364 - val_mse: 0.1364\n",
      "Epoch 124/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0108 - mse: 0.0108 - val_loss: 0.1392 - val_mse: 0.1392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0110 - mse: 0.0110 - val_loss: 0.1346 - val_mse: 0.1346\n",
      "Epoch 126/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0107 - mse: 0.0107 - val_loss: 0.1364 - val_mse: 0.1364\n",
      "Epoch 127/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 128/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 129/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.1355 - val_mse: 0.1355\n",
      "Epoch 130/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.1356 - val_mse: 0.1356\n",
      "Epoch 131/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1356 - val_mse: 0.1356\n",
      "Epoch 132/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0103 - mse: 0.0103 - val_loss: 0.1365 - val_mse: 0.1365\n",
      "Epoch 133/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1364 - val_mse: 0.1364\n",
      "Epoch 134/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1364 - val_mse: 0.1364\n",
      "Epoch 135/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.1362 - val_mse: 0.1362\n",
      "Epoch 136/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.1354 - val_mse: 0.1354\n",
      "Epoch 137/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.1359 - val_mse: 0.1359\n",
      "Epoch 138/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.1354 - val_mse: 0.1354\n",
      "Epoch 139/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 140/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0093 - mse: 0.0093 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 141/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.1372 - val_mse: 0.1372\n",
      "Epoch 142/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 143/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.1369 - val_mse: 0.1369\n",
      "Epoch 144/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.1374 - val_mse: 0.1374\n",
      "Epoch 145/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.1364 - val_mse: 0.1364\n",
      "Epoch 146/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.1355 - val_mse: 0.1355\n",
      "Epoch 147/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.1350 - val_mse: 0.1350\n",
      "Epoch 148/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.1355 - val_mse: 0.1355\n",
      "Epoch 149/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.1357 - val_mse: 0.1357\n",
      "Epoch 150/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.1363 - val_mse: 0.1363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2744124580>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "lecun_model = Sequential()\n",
    "\n",
    "# Add the first hidden layer\n",
    "lecun_model.add(layers.Dense(100, activation='relu', \n",
    "                             input_shape=(n_features,), \n",
    "                             kernel_initializer='lecun_normal'))\n",
    "\n",
    "# Add another hidden layer\n",
    "lecun_model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Add an output layer\n",
    "lecun_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "lecun_model.compile(optimizer='SGD', \n",
    "                    loss='mse', \n",
    "                    metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "lecun_model.fit(X_train, \n",
    "                y_train_scaled, \n",
    "                batch_size=32, \n",
    "                epochs=150, \n",
    "                validation_data=(X_val, y_val_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`lecun_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 484us/step - loss: 0.0079 - mse: 0.0079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00788559578359127, 0.00788559578359127]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "lecun_model.evaluate(X_train, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`lecun_model`) on validate data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 530us/step - loss: 0.1363 - mse: 0.1363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1362691968679428, 0.1362691968679428]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on validate data\n",
    "lecun_model.evaluate(X_val, y_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a difference, but a useful note to consider when tuning your network. Next, let's investigate the impact of various optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop \n",
    "\n",
    "Compile the `rmsprop_model` with: \n",
    "\n",
    "- `'rmsprop'` as the optimizer \n",
    "- track `'mse'` as the loss and metric  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.3225 - mse: 0.3225 - val_loss: 0.1107 - val_mse: 0.1107\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1673 - mse: 0.1673 - val_loss: 0.0868 - val_mse: 0.0868\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1224 - mse: 0.1224 - val_loss: 0.1148 - val_mse: 0.1148\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0884 - mse: 0.0884 - val_loss: 0.1105 - val_mse: 0.1105\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0813 - mse: 0.0813 - val_loss: 0.1223 - val_mse: 0.1223\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0626 - mse: 0.0626 - val_loss: 0.1247 - val_mse: 0.1247\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0578 - mse: 0.0578 - val_loss: 0.1706 - val_mse: 0.1706\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0500 - mse: 0.0500 - val_loss: 0.1025 - val_mse: 0.1025\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0434 - mse: 0.0434 - val_loss: 0.1623 - val_mse: 0.1623\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0399 - mse: 0.0399 - val_loss: 0.1497 - val_mse: 0.1497\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0361 - mse: 0.0361 - val_loss: 0.1381 - val_mse: 0.1381\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0302 - mse: 0.0302 - val_loss: 0.1333 - val_mse: 0.1333\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1150 - val_mse: 0.1150\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.1386 - val_mse: 0.1386\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.1203 - val_mse: 0.1203\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0294 - mse: 0.0294 - val_loss: 0.1307 - val_mse: 0.1307\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0995 - val_mse: 0.0995\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0221 - mse: 0.0221 - val_loss: 0.1359 - val_mse: 0.1359\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0161 - mse: 0.0161 - val_loss: 0.1063 - val_mse: 0.1063\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.1467 - val_mse: 0.1467\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0168 - mse: 0.0168 - val_loss: 0.1042 - val_mse: 0.1042\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.1306 - val_mse: 0.1306\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0170 - mse: 0.0170 - val_loss: 0.0985 - val_mse: 0.0985\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.1473 - val_mse: 0.1473\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0163 - mse: 0.0163 - val_loss: 0.1021 - val_mse: 0.1021\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0167 - mse: 0.0167 - val_loss: 0.1338 - val_mse: 0.1338\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0973 - val_mse: 0.0973\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0137 - mse: 0.0137 - val_loss: 0.1230 - val_mse: 0.1230\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0126 - mse: 0.0126 - val_loss: 0.1080 - val_mse: 0.1080\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.1100 - val_mse: 0.1100\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.1332 - val_mse: 0.1332\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.1046 - val_mse: 0.1046\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.1064 - val_mse: 0.1064\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0119 - mse: 0.0119 - val_loss: 0.1175 - val_mse: 0.1175\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0121 - mse: 0.0121 - val_loss: 0.1035 - val_mse: 0.1035\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0120 - mse: 0.0120 - val_loss: 0.1110 - val_mse: 0.1110\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.0997 - val_mse: 0.0997\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.1086 - val_mse: 0.1086\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0934 - val_mse: 0.0934\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.1235 - val_mse: 0.1235\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1047 - val_mse: 0.1047\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.1033 - val_mse: 0.1033\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0907 - val_mse: 0.0907\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.1155 - val_mse: 0.1155\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0104 - mse: 0.0104 - val_loss: 0.1033 - val_mse: 0.1033\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.1144 - val_mse: 0.1144\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0106 - mse: 0.0106 - val_loss: 0.0862 - val_mse: 0.0862\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0089 - mse: 0.0089 - val_loss: 0.1075 - val_mse: 0.1075\n",
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0097 - mse: 0.0097 - val_loss: 0.1049 - val_mse: 0.1049\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0080 - mse: 0.0080 - val_loss: 0.1014 - val_mse: 0.1014\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0974 - val_mse: 0.0974\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.1025 - val_mse: 0.1025\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0094 - mse: 0.0094 - val_loss: 0.0914 - val_mse: 0.0914\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.1052 - val_mse: 0.1052\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0085 - mse: 0.0085 - val_loss: 0.0860 - val_mse: 0.0860\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.1011 - val_mse: 0.1011\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0828 - val_mse: 0.0828\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.1109 - val_mse: 0.1109\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0935 - val_mse: 0.0935\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0900 - val_mse: 0.0900\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0980 - val_mse: 0.0980\n",
      "Epoch 63/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0877 - val_mse: 0.0877\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0944 - val_mse: 0.0944\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0945 - val_mse: 0.0945\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.1139 - val_mse: 0.1139\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0078 - mse: 0.0078 - val_loss: 0.0882 - val_mse: 0.0882\n",
      "Epoch 69/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.1044 - val_mse: 0.1044\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0854 - val_mse: 0.0854\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0991 - val_mse: 0.0991\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0066 - mse: 0.0066 - val_loss: 0.0764 - val_mse: 0.0764\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.1126 - val_mse: 0.1126\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0069 - mse: 0.0069 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0055 - mse: 0.0055 - val_loss: 0.0948 - val_mse: 0.0948\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0829 - val_mse: 0.0829\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0887 - val_mse: 0.0887\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0836 - val_mse: 0.0836\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0925 - val_mse: 0.0925\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0845 - val_mse: 0.0845\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0950 - val_mse: 0.0950\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0831 - val_mse: 0.0831\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0825 - val_mse: 0.0825\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0063 - mse: 0.0063 - val_loss: 0.0938 - val_mse: 0.0938\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0965 - val_mse: 0.0965\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0876 - val_mse: 0.0876\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0847 - val_mse: 0.0847\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0851 - val_mse: 0.0851\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0855 - val_mse: 0.0855\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0888 - val_mse: 0.0888\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0897 - val_mse: 0.0897\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0901 - val_mse: 0.0901\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0942 - val_mse: 0.0942\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0912 - val_mse: 0.0912\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0945 - val_mse: 0.0945\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.0906 - val_mse: 0.0906\n",
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0057 - mse: 0.0057 - val_loss: 0.0882 - val_mse: 0.0882\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0065 - mse: 0.0065 - val_loss: 0.0868 - val_mse: 0.0868\n",
      "Epoch 101/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0060 - mse: 0.0060 - val_loss: 0.0892 - val_mse: 0.0892\n",
      "Epoch 102/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0839 - val_mse: 0.0839\n",
      "Epoch 103/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0062 - mse: 0.0062 - val_loss: 0.0971 - val_mse: 0.0971\n",
      "Epoch 104/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0048 - mse: 0.0048 - val_loss: 0.0870 - val_mse: 0.0870\n",
      "Epoch 105/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0972 - val_mse: 0.0972\n",
      "Epoch 106/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0816 - val_mse: 0.0816\n",
      "Epoch 107/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0054 - mse: 0.0054 - val_loss: 0.0929 - val_mse: 0.0929\n",
      "Epoch 108/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0052 - mse: 0.0052 - val_loss: 0.0789 - val_mse: 0.0789\n",
      "Epoch 109/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0925 - val_mse: 0.0925\n",
      "Epoch 110/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0047 - mse: 0.0047 - val_loss: 0.0817 - val_mse: 0.0817\n",
      "Epoch 111/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0819 - val_mse: 0.0819\n",
      "Epoch 112/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0799 - val_mse: 0.0799\n",
      "Epoch 113/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0053 - mse: 0.0053 - val_loss: 0.0846 - val_mse: 0.0846\n",
      "Epoch 114/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0803 - val_mse: 0.0803\n",
      "Epoch 115/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0906 - val_mse: 0.0906\n",
      "Epoch 116/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0059 - mse: 0.0059 - val_loss: 0.0822 - val_mse: 0.0822\n",
      "Epoch 117/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0884 - val_mse: 0.0884\n",
      "Epoch 118/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0764 - val_mse: 0.0764\n",
      "Epoch 119/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0061 - mse: 0.0061 - val_loss: 0.0800 - val_mse: 0.0800\n",
      "Epoch 120/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0832 - val_mse: 0.0832\n",
      "Epoch 121/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0795 - val_mse: 0.0795\n",
      "Epoch 122/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0852 - val_mse: 0.0852\n",
      "Epoch 123/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0841 - val_mse: 0.0841\n",
      "Epoch 124/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0051 - mse: 0.0051 - val_loss: 0.0837 - val_mse: 0.0837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0820 - val_mse: 0.0820\n",
      "Epoch 126/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0819 - val_mse: 0.0819\n",
      "Epoch 127/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0823 - val_mse: 0.0823\n",
      "Epoch 128/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.0831 - val_mse: 0.0831\n",
      "Epoch 129/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0865 - val_mse: 0.0865\n",
      "Epoch 130/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0829 - val_mse: 0.0829\n",
      "Epoch 131/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0907 - val_mse: 0.0907\n",
      "Epoch 132/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0056 - mse: 0.0056 - val_loss: 0.0766 - val_mse: 0.0766\n",
      "Epoch 133/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0855 - val_mse: 0.0855\n",
      "Epoch 134/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0824 - val_mse: 0.0824\n",
      "Epoch 135/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0898 - val_mse: 0.0898\n",
      "Epoch 136/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0847 - val_mse: 0.0847\n",
      "Epoch 137/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0855 - val_mse: 0.0855\n",
      "Epoch 138/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.0815 - val_mse: 0.0815\n",
      "Epoch 139/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0901 - val_mse: 0.0901\n",
      "Epoch 140/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0917 - val_mse: 0.0917\n",
      "Epoch 141/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 142/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.0873 - val_mse: 0.0873\n",
      "Epoch 143/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0870 - val_mse: 0.0870\n",
      "Epoch 144/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0818 - val_mse: 0.0818\n",
      "Epoch 145/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0895 - val_mse: 0.0895\n",
      "Epoch 146/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0818 - val_mse: 0.0818\n",
      "Epoch 147/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0919 - val_mse: 0.0919\n",
      "Epoch 148/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0050 - mse: 0.0050 - val_loss: 0.0798 - val_mse: 0.0798\n",
      "Epoch 149/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0044 - mse: 0.0044 - val_loss: 0.0941 - val_mse: 0.0941\n",
      "Epoch 150/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.0833 - val_mse: 0.0833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f272c33a190>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "rmsprop_model = Sequential()\n",
    "rmsprop_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "rmsprop_model.add(layers.Dense(50, activation='relu'))\n",
    "rmsprop_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "rmsprop_model.compile(optimizer='rmsprop', \n",
    "                      loss='mse',\n",
    "                      metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "rmsprop_model.fit(X_train, \n",
    "                  y_train_scaled, \n",
    "                  batch_size=32, \n",
    "                  epochs=150, \n",
    "                  validation_data=(X_val, y_val_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`rmsprop_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 478us/step - loss: 0.0041 - mse: 0.0041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0041152616031467915, 0.0041152616031467915]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "rmsprop_model.evaluate(X_train, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`rmsprop_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 536us/step - loss: 0.0833 - mse: 0.0833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08330497145652771, 0.08330497145652771]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on validate data\n",
    "rmsprop_model.evaluate(X_val, y_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam \n",
    "\n",
    "Compile the `adam_model` with: \n",
    "\n",
    "- `'Adam'` as the optimizer \n",
    "- track `'mse'` as the loss and metric  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.3668 - mse: 0.3668 - val_loss: 0.1448 - val_mse: 0.1448\n",
      "Epoch 2/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1552 - mse: 0.1552 - val_loss: 0.1114 - val_mse: 0.1114\n",
      "Epoch 3/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1157 - mse: 0.1157 - val_loss: 0.1047 - val_mse: 0.1047\n",
      "Epoch 4/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0824 - mse: 0.0824 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 5/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0641 - mse: 0.0641 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "Epoch 6/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0491 - mse: 0.0491 - val_loss: 0.0935 - val_mse: 0.0935\n",
      "Epoch 7/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0370 - mse: 0.0370 - val_loss: 0.0951 - val_mse: 0.0951\n",
      "Epoch 8/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0287 - mse: 0.0287 - val_loss: 0.0987 - val_mse: 0.0987\n",
      "Epoch 9/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0972 - val_mse: 0.0972\n",
      "Epoch 10/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0212 - mse: 0.0212 - val_loss: 0.0997 - val_mse: 0.0997\n",
      "Epoch 11/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0989 - val_mse: 0.0989\n",
      "Epoch 12/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0149 - mse: 0.0149 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 13/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.1100 - val_mse: 0.1100\n",
      "Epoch 14/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0131 - mse: 0.0131 - val_loss: 0.1006 - val_mse: 0.1006\n",
      "Epoch 15/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0134 - mse: 0.0134 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 16/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0147 - mse: 0.0147 - val_loss: 0.1021 - val_mse: 0.1021\n",
      "Epoch 17/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.1138 - val_mse: 0.1138\n",
      "Epoch 18/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0157 - mse: 0.0157 - val_loss: 0.0936 - val_mse: 0.0936\n",
      "Epoch 19/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0114 - mse: 0.0114 - val_loss: 0.1050 - val_mse: 0.1050\n",
      "Epoch 20/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0115 - mse: 0.0115 - val_loss: 0.0988 - val_mse: 0.0988\n",
      "Epoch 21/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0111 - mse: 0.0111 - val_loss: 0.1142 - val_mse: 0.1142\n",
      "Epoch 22/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0099 - mse: 0.0099 - val_loss: 0.1020 - val_mse: 0.1020\n",
      "Epoch 23/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0196 - mse: 0.0196 - val_loss: 0.1126 - val_mse: 0.1126\n",
      "Epoch 24/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0229 - mse: 0.0229 - val_loss: 0.1035 - val_mse: 0.1035\n",
      "Epoch 25/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0432 - mse: 0.0432 - val_loss: 0.1064 - val_mse: 0.1064\n",
      "Epoch 26/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0152 - mse: 0.0152 - val_loss: 0.1035 - val_mse: 0.1035\n",
      "Epoch 27/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0929 - val_mse: 0.0929\n",
      "Epoch 28/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.1063 - val_mse: 0.1063\n",
      "Epoch 29/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.1001 - val_mse: 0.1001\n",
      "Epoch 30/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.1033 - val_mse: 0.1033\n",
      "Epoch 31/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.1007 - val_mse: 0.1007\n",
      "Epoch 32/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.1055 - val_mse: 0.1055\n",
      "Epoch 33/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.1029 - val_mse: 0.1029\n",
      "Epoch 34/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.1058 - val_mse: 0.1058\n",
      "Epoch 35/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.1015 - val_mse: 0.1015\n",
      "Epoch 36/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0996 - val_mse: 0.0996\n",
      "Epoch 37/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.1010 - val_mse: 0.1010\n",
      "Epoch 38/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0983 - val_mse: 0.0983\n",
      "Epoch 39/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.1016 - val_mse: 0.1016\n",
      "Epoch 40/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.1015 - val_mse: 0.1015\n",
      "Epoch 41/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.1030 - val_mse: 0.1030\n",
      "Epoch 42/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.1016 - val_mse: 0.1016\n",
      "Epoch 43/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0019 - mse: 0.0019 - val_loss: 0.1021 - val_mse: 0.1021\n",
      "Epoch 44/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.1014 - val_mse: 0.1014\n",
      "Epoch 45/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.1016 - val_mse: 0.1016\n",
      "Epoch 46/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.1013 - val_mse: 0.1013\n",
      "Epoch 47/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.1009 - val_mse: 0.1009\n",
      "Epoch 48/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 7.0016e-04 - mse: 7.0016e-04 - val_loss: 0.1008 - val_mse: 0.1008\n",
      "Epoch 49/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 5.9371e-04 - mse: 5.9371e-04 - val_loss: 0.1019 - val_mse: 0.1019\n",
      "Epoch 50/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 5.7502e-04 - mse: 5.7502e-04 - val_loss: 0.1011 - val_mse: 0.1011\n",
      "Epoch 51/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 5.5739e-04 - mse: 5.5739e-04 - val_loss: 0.1021 - val_mse: 0.1021\n",
      "Epoch 52/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 5.7804e-04 - mse: 5.7804e-04 - val_loss: 0.1008 - val_mse: 0.1008\n",
      "Epoch 53/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 8.7649e-04 - mse: 8.7649e-04 - val_loss: 0.1015 - val_mse: 0.1015\n",
      "Epoch 54/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.1003 - val_mse: 0.1003\n",
      "Epoch 55/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.1045 - val_mse: 0.1045\n",
      "Epoch 56/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.1008 - val_mse: 0.1008\n",
      "Epoch 57/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0020 - mse: 0.0020 - val_loss: 0.1056 - val_mse: 0.1056\n",
      "Epoch 58/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0992 - val_mse: 0.0992\n",
      "Epoch 59/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0036 - mse: 0.0036 - val_loss: 0.1064 - val_mse: 0.1064\n",
      "Epoch 60/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.1021 - val_mse: 0.1021\n",
      "Epoch 61/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.1037 - val_mse: 0.1037\n",
      "Epoch 62/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.1010 - val_mse: 0.1010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0041 - mse: 0.0041 - val_loss: 0.1002 - val_mse: 0.1002\n",
      "Epoch 64/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.1106 - val_mse: 0.1106\n",
      "Epoch 65/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.1004 - val_mse: 0.1004\n",
      "Epoch 66/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.1045 - val_mse: 0.1045\n",
      "Epoch 67/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.1011 - val_mse: 0.1011\n",
      "Epoch 68/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0064 - mse: 0.0064 - val_loss: 0.1012 - val_mse: 0.1012\n",
      "Epoch 69/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0042 - mse: 0.0042 - val_loss: 0.0933 - val_mse: 0.0933\n",
      "Epoch 70/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0948 - val_mse: 0.0948\n",
      "Epoch 71/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0029 - mse: 0.0029 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 72/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0991 - val_mse: 0.0991\n",
      "Epoch 73/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 74/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0991 - val_mse: 0.0991\n",
      "Epoch 75/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0972 - val_mse: 0.0972\n",
      "Epoch 76/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0032 - mse: 0.0032 - val_loss: 0.0977 - val_mse: 0.0977\n",
      "Epoch 77/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0030 - mse: 0.0030 - val_loss: 0.0967 - val_mse: 0.0967\n",
      "Epoch 78/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.0982 - val_mse: 0.0982\n",
      "Epoch 79/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 80/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0971 - val_mse: 0.0971\n",
      "Epoch 81/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.0028 - mse: 0.0028 - val_loss: 0.0981 - val_mse: 0.0981\n",
      "Epoch 82/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0905 - val_mse: 0.0905\n",
      "Epoch 83/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0949 - val_mse: 0.0949\n",
      "Epoch 84/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0016 - mse: 0.0016 - val_loss: 0.0930 - val_mse: 0.0930\n",
      "Epoch 85/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.1002 - val_mse: 0.1002\n",
      "Epoch 86/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0925 - val_mse: 0.0925\n",
      "Epoch 87/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 9.8476e-04 - mse: 9.8476e-04 - val_loss: 0.0974 - val_mse: 0.0974\n",
      "Epoch 88/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0943 - val_mse: 0.0943\n",
      "Epoch 89/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0987 - val_mse: 0.0987\n",
      "Epoch 90/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0897 - val_mse: 0.0897\n",
      "Epoch 91/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0970 - val_mse: 0.0970\n",
      "Epoch 92/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0049 - mse: 0.0049 - val_loss: 0.0952 - val_mse: 0.0952\n",
      "Epoch 93/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0122 - mse: 0.0122 - val_loss: 0.1087 - val_mse: 0.1087\n",
      "Epoch 94/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0138 - mse: 0.0138 - val_loss: 0.0915 - val_mse: 0.0915\n",
      "Epoch 95/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.1179 - val_mse: 0.1179\n",
      "Epoch 96/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0159 - mse: 0.0159 - val_loss: 0.0894 - val_mse: 0.0894\n",
      "Epoch 97/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.1075 - val_mse: 0.1075\n",
      "Epoch 98/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0129 - mse: 0.0129 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 99/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0920 - val_mse: 0.0920\n",
      "Epoch 100/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0914 - val_mse: 0.0914\n",
      "Epoch 101/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 102/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0910 - val_mse: 0.0910\n",
      "Epoch 103/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0914 - val_mse: 0.0914\n",
      "Epoch 104/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 9.8091e-04 - mse: 9.8091e-04 - val_loss: 0.0928 - val_mse: 0.0928\n",
      "Epoch 105/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 6.2247e-04 - mse: 6.2247e-04 - val_loss: 0.0932 - val_mse: 0.0932\n",
      "Epoch 106/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 3.5794e-04 - mse: 3.5794e-04 - val_loss: 0.0921 - val_mse: 0.0921\n",
      "Epoch 107/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 2.5758e-04 - mse: 2.5758e-04 - val_loss: 0.0928 - val_mse: 0.0928\n",
      "Epoch 108/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.7999e-04 - mse: 1.7999e-04 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 109/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.5151e-04 - mse: 1.5151e-04 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 110/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.2224e-04 - mse: 1.2224e-04 - val_loss: 0.0928 - val_mse: 0.0928\n",
      "Epoch 111/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.1432e-04 - mse: 1.1432e-04 - val_loss: 0.0921 - val_mse: 0.0921\n",
      "Epoch 112/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.2335e-04 - mse: 1.2335e-04 - val_loss: 0.0921 - val_mse: 0.0921\n",
      "Epoch 113/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0593e-04 - mse: 1.0593e-04 - val_loss: 0.0926 - val_mse: 0.0926\n",
      "Epoch 114/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0005e-04 - mse: 1.0005e-04 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 115/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0933e-04 - mse: 1.0933e-04 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 116/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.2079e-04 - mse: 1.2079e-04 - val_loss: 0.0949 - val_mse: 0.0949\n",
      "Epoch 117/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 2.1456e-04 - mse: 2.1456e-04 - val_loss: 0.0919 - val_mse: 0.0919\n",
      "Epoch 118/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 2.5315e-04 - mse: 2.5315e-04 - val_loss: 0.0930 - val_mse: 0.0930\n",
      "Epoch 119/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 2.4642e-04 - mse: 2.4642e-04 - val_loss: 0.0929 - val_mse: 0.0929\n",
      "Epoch 120/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 2.7581e-04 - mse: 2.7581e-04 - val_loss: 0.0918 - val_mse: 0.0918\n",
      "Epoch 121/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 3.9205e-04 - mse: 3.9205e-04 - val_loss: 0.0939 - val_mse: 0.0939\n",
      "Epoch 122/150\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 4.6150e-04 - mse: 4.6150e-04 - val_loss: 0.0925 - val_mse: 0.0925\n",
      "Epoch 123/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step - loss: 6.5357e-04 - mse: 6.5357e-04 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 124/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 7.6071e-04 - mse: 7.6071e-04 - val_loss: 0.0917 - val_mse: 0.0917\n",
      "Epoch 125/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 126/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0021 - mse: 0.0021 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 127/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0986 - val_mse: 0.0986\n",
      "Epoch 128/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0898 - val_mse: 0.0898\n",
      "Epoch 129/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0046 - mse: 0.0046 - val_loss: 0.1035 - val_mse: 0.1035\n",
      "Epoch 130/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0043 - mse: 0.0043 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 131/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0058 - mse: 0.0058 - val_loss: 0.1007 - val_mse: 0.1007\n",
      "Epoch 132/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0045 - mse: 0.0045 - val_loss: 0.0894 - val_mse: 0.0894\n",
      "Epoch 133/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0035 - mse: 0.0035 - val_loss: 0.0998 - val_mse: 0.0998\n",
      "Epoch 134/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0038 - mse: 0.0038 - val_loss: 0.0917 - val_mse: 0.0917\n",
      "Epoch 135/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0037 - mse: 0.0037 - val_loss: 0.0916 - val_mse: 0.0916\n",
      "Epoch 136/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0040 - mse: 0.0040 - val_loss: 0.0907 - val_mse: 0.0907\n",
      "Epoch 137/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0033 - mse: 0.0033 - val_loss: 0.0919 - val_mse: 0.0919\n",
      "Epoch 138/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0921 - val_mse: 0.0921\n",
      "Epoch 139/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0023 - mse: 0.0023 - val_loss: 0.0927 - val_mse: 0.0927\n",
      "Epoch 140/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0017 - mse: 0.0017 - val_loss: 0.0893 - val_mse: 0.0893\n",
      "Epoch 141/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0962 - val_mse: 0.0962\n",
      "Epoch 142/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0922 - val_mse: 0.0922\n",
      "Epoch 143/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 144/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0872 - val_mse: 0.0872\n",
      "Epoch 145/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0022 - mse: 0.0022 - val_loss: 0.0951 - val_mse: 0.0951\n",
      "Epoch 146/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0027 - mse: 0.0027 - val_loss: 0.0866 - val_mse: 0.0866\n",
      "Epoch 147/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0024 - mse: 0.0024 - val_loss: 0.0876 - val_mse: 0.0876\n",
      "Epoch 148/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0026 - mse: 0.0026 - val_loss: 0.0924 - val_mse: 0.0924\n",
      "Epoch 149/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0031 - mse: 0.0031 - val_loss: 0.0895 - val_mse: 0.0895\n",
      "Epoch 150/150\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.0039 - mse: 0.0039 - val_loss: 0.1004 - val_mse: 0.1004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f272c1601c0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "adam_model = Sequential()\n",
    "adam_model.add(layers.Dense(100, activation='relu', input_shape=(n_features,)))\n",
    "adam_model.add(layers.Dense(50, activation='relu'))\n",
    "adam_model.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model\n",
    "adam_model.compile(optimizer='Adam', loss='mse', metrics=['mse'])\n",
    "\n",
    "# Train the model\n",
    "adam_model.fit(X_train, \n",
    "               y_train_scaled, \n",
    "               batch_size=32, \n",
    "               epochs=150, \n",
    "               validation_data=(X_val, y_val_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`adam_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 489us/step - loss: 0.0051 - mse: 0.0051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.005076603032648563, 0.005076603032648563]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "adam_model.evaluate(X_train, y_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model (`adam_model`) on training data (`X_train` and `y_train_scaled`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 557us/step - loss: 0.1004 - mse: 0.1004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10037045925855637, 0.10037045925855637]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on validate data\n",
    "adam_model.evaluate(X_val, y_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a Final Model\n",
    "\n",
    "Now, select the model with the best performance based on the training and validation sets. Evaluate this top model using the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 591us/step - loss: 0.1793 - mse: 0.1793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17930561304092407, 0.17930561304092407]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the best model on test data\n",
    "rmsprop_model.evaluate(X_test, y_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As earlier, this metric is hard to interpret because the output is scaled. \n",
    "\n",
    "- Generate predictions on test data (`X_test`) \n",
    "- Transform these predictions back to original scale using `ss_y` \n",
    "- Now you can calculate the RMSE in the original units with `y_test` and `y_test_pred` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33275.79275712494"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions on test data\n",
    "y_test_pred_scaled = rmsprop_model.predict(X_test)\n",
    "\n",
    "# Transform the predictions back to original scale\n",
    "y_test_pred = ss_y.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "# MSE of test data\n",
    "mean_squared_error(y_test, y_test_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lab, you worked to ensure your model converged properly by normalizing both the input and output. Additionally, you also investigated the impact of varying initialization and optimization routines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
